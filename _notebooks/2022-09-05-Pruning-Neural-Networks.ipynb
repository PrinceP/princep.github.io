{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Make Neural Networks Faster\"\n",
    "> \"Methods for compressing and accelerating deep learning models - Papers for each task's\"\n",
    "\n",
    "- toc: true\n",
    "- comments: false\n",
    "- categories: [Papers]\n",
    "- image: images/brain_logo.png\n",
    "- author: Prince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications\n",
    "---\n",
    "\n",
    "- [Natural Language Processing with Small Feed-Forward Networks](https://arxiv.org/abs/1708.00214)\n",
    "- [Machine Learning at Facebook: Understanding Inference at the Edge](https://research.fb.com/wp-content/uploads/2018/12/Machine-Learning-at-Facebook-Understanding-Inference-at-the-Edge.pdf)\n",
    "- [Recognizing People in Photos Through Private On-Device Machine Learning](https://machinelearning.apple.com/research/recognizing-people-photos)\n",
    "- [Knowledge Transfer for Efficient On-device False Trigger Mitigation](https://arxiv.org/abs/2010.10591)\n",
    "- [Smart Reply: Automated Response Suggestion for Email](https://arxiv.org/abs/1606.04870)\n",
    "- [Chat Smarter with Allo](https://ai.googleblog.com/2016/05/chat-smarter-with-allo.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation\n",
    "---\n",
    "\n",
    "- [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)\n",
    "- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
    "- [TinyBERT: Distilling BERT for Natural Language Understanding](https://aclanthology.org/2020.findings-emnlp.372/)\n",
    "- [DistilBERT](https://arxiv.org/abs/1910.01108)\n",
    "- [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://www.aclweb.org/anthology/2020.acl-main.195/)\n",
    "- [Distilling Large Language Models into Tiny and Effective Students using pQRNN](https://arxiv.org/abs/2101.08890)\n",
    "- [Sequence-Level Knowledge Distillation](https://arxiv.org/abs/1606.07947)\n",
    "- [DynaBERT: Dynamic BERT with Adaptive Width and Depth](https://arxiv.org/abs/2004.04037)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning\n",
    "----\n",
    "- [Optimal Brain Damage](https://papers.nips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)\n",
    "- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)\n",
    "- [The Lottery Ticket Hypothesis: A Survey (blog post)](https://roberttlange.github.io/posts/2020/06/lottery-ticket-hypothesis/)\n",
    "- [Bayesian Bits: Unifying Quantization and Pruning](https://arxiv.org/abs/2005.07093)\n",
    "- [Structured Pruning of Neural Networks with Budget-Aware Regularization](https://ieeexplore.ieee.org/abstract/document/8953545)\n",
    "- [Block Pruning For Faster Transformers](https://aclanthology.org/2021.emnlp-main.829/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural architecture search\n",
    "----\n",
    "- [SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers](https://arxiv.org/abs/1905.12107)\n",
    "- [FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable NAS](https://arxiv.org/abs/1812.03443)\n",
    "- [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)\n",
    "- [High-Performance Large-Scale Image Recognition Without Normalization](https://arxiv.org/abs/2102.06171)\n",
    "- [HAT: Hardware-Aware Transformers for Efficient Natural Language Processing](https://arxiv.org/abs/2005.14187)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "----\n",
    "- [Show Your Work: Improved Reporting of Experimental Results](https://aclanthology.org/D19-1224/)\n",
    "- [Showing Your Work Doesn’t Always Work](https://aclanthology.org/2020.acl-main.246/)\n",
    "- [The Hardware Lottery](https://arxiv.org/abs/2009.06489)\n",
    "- [HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing](https://arxiv.org/abs/2002.05829)\n",
    "- [An Analysis of Deep Neural Network Models for Practical Applications](https://arxiv.org/abs/1605.07678)\n",
    "- [MLPerf Inference Benchmark](https://arxiv.org/abs/1911.02549)\n",
    "- [MLPerf Training Benchmark](https://arxiv.org/abs/1910.01500)\n",
    "- [Roofline: an insightful visual performance model for multicore architectures](https://people.eecs.berkeley.edu/~kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf)\n",
    "- [Evaluating the Energy Efficiency of Deep Convolutional Neural Networks on CPUs and GPUs](https://ieeexplore.ieee.org/document/7723730)\n",
    "- [Deep Learning Language Modeling Workloads: Where Time Goes on Graphics Processors](https://ieeexplore.ieee.org/document/9041972)\n",
    "- [Energy and Policy Considerations for Deep Learning in NLP](https://aclanthology.org/P19-1355/)\n",
    "- [IrEne: Interpretable Energy Prediction for Transformers](https://aclanthology.org/2021.acl-long.167/)\n",
    "- [Measuring the Carbon Intensity of AI in Cloud Instances](https://dl.acm.org/doi/10.1145/3531146.3533234)\n",
    "- [Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning](https://jmlr.org/papers/v21/20-312.html)\n",
    "- [Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models](https://arxiv.org/abs/2007.03051)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "----\n",
    "- [Scalable Methods for 8-bit Training of Neural Networks](https://arxiv.org/abs/1805.11046)\n",
    "- [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](https://arxiv.org/abs/2002.11794)\n",
    "- [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://arxiv.org/abs/1908.09791)\n",
    "- [Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://arxiv.org/abs/1909.05840)\n",
    "- [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321)\n",
    "- [BinaryBERT](https://aclanthology.org/2021.acl-long.334/)\n",
    "- [TernaryBERT: Distillation-aware Ultra-low Bit BERT](https://www.aclweb.org/anthology/2020.emnlp-main.37/)\n",
    "- [Binarized Neural Networks](https://arxiv.org/abs/1602.02830)\n",
    "- [Training Deep Neural Networks with 8-bit Floating Point Numbers](https://arxiv.org/abs/1812.08011)\n",
    "- [HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision](https://arxiv.org/abs/1905.03696)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Accelerating training\n",
    "----\n",
    "- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)\n",
    "- [Pre-Training Transformers as Energy-Based Cloze Models](https://arxiv.org/abs/2012.08561)\n",
    "- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)\n",
    "- [Accelerating Deep Learning by Focusing on the Biggest Losers](https://arxiv.org/abs/1910.00762)\n",
    "- [Dataset Distillation](https://arxiv.org/abs/1811.10959)\n",
    "- [Competence-based curriculum learning for neural machine translation](https://arxiv.org/abs/1903.09848)\n",
    "- [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal\n",
    "----\n",
    "- [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557)\n",
    "- [Mapping Navigation Instructions to Cont. Control Actions with Position-Visitation Pred.](https://arxiv.org/abs/1811.04179)\n",
    "- [Early Fusion for Goal Directed Robotic Vision](https://arxiv.org/abs/1811.08824)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-specific tricks\n",
    "----\n",
    "- [A Study of Non-autoregressive Model for Sequence Generation](https://arxiv.org/abs/2004.10454)\n",
    "- [Mask-Predict: Parallel Decoding of Conditional Masked Language Models](https://arxiv.org/abs/1904.09324)\n",
    "- [Non-autoregressive neural machine translation](https://arxiv.org/abs/1711.02281)\n",
    "- [Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation](https://arxiv.org/abs/2006.10369)\n",
    "- [Improving Low Compute Language Modeling with In-Domain Embedding Initialisation](https://arxiv.org/abs/2009.14109)\n",
    "- [COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List](https://aclanthology.org/2021.naacl-main.241/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Architecture Specific Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CNNs\n",
    "---\n",
    "- [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279)\n",
    "- [XOR-Net: An Efficient Computation Pipeline for Binary Neural Network Inference on Edge Devices](https://ieeexplore.ieee.org/document/9359148)\n",
    "- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)\n",
    "- [Fast Convolutional Nets With fbfft: A GPU Performance Evaluation](https://arxiv.org/abs/1412.7580)\n",
    "- [FFT Convolutions are Faster than Winograd on Modern CPUs, Here’s Why](https://arxiv.org/abs/1809.07851)\n",
    "- [Fast Algorithms for Convolutional Neural Networks](https://arxiv.org/abs/1509.09308)\n",
    "\n",
    "## Softmax\n",
    "----\n",
    "- [Efficient softmax approximation for GPUs](https://arxiv.org/abs/1609.04309)\n",
    "\n",
    "## Embeddings/inputs\n",
    "----\n",
    "- [Adaptive Input Representations for Neural Language Modeling](https://arxiv.org/abs/1809.1085)\n",
    "\n",
    "## Transformers\n",
    "----\n",
    "- [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236)\n",
    "- [Do Transformer Modifications Transfer Across Implementations and Applications?](https://arxiv.org/abs/2102.11972)\n",
    "- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)\n",
    "- [Consistent Accelerated Inference via Confident Adaptive Transformers](https://arxiv.org/abs/2104.08803)\n",
    "- [PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination](https://arxiv.org/abs/2001.08950)\n",
    "- [Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level](https://arxiv.org/abs/2105.06020)\n",
    "- [Are Sixteen Heads Really Better Than One?](http://papers.nips.cc/paper/9551-are-sixteen-heads-really-better-than-one)\n",
    "- [Are Pre-trained Convolutions Better than Pre-trained Transformers?](https://aclanthology.org/2021.acl-long.335/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech\n",
    "----\n",
    "- [Speech recognition with deep recurrent neural networks](https://ieeexplore.ieee.org/abstract/document/6638947)\n",
    "- [Streaming End-to-end Speech Recognition For Mobile Devices](https://arxiv.org/abs/1811.06621)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carbon footprint and alternative power sources\n",
    "----\n",
    "- [Tackling Climate Change with Machine Learning](https://arxiv.org/abs/1906.05433)\n",
    "- [On the opportunities and risks of foundation models (Section 5.3)](https://arxiv.org/abs/2108.07258)\n",
    "- [Quantifying the Carbon Emissions of Machine Learning](https://arxiv.org/abs/1910.09700)\n",
    "- [AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning](https://arxiv.org/abs/2107.08147)\n",
    "\n",
    "# New papers\n",
    "----\n",
    "- [Structured Pruning Learns Compact and Accurate Models](https://aclanthology.org/2022.acl-long.107/)\n",
    "- [Embedding Recycling for Language Models](https://arxiv.org/abs/2207.04993)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
