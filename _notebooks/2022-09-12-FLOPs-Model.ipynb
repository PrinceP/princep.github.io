{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc576bb-cb8e-476c-8361-c0376bf11c46",
   "metadata": {},
   "source": [
    "# \"Benchmarking Neural Networks\"\n",
    "> \"Methods for calculating the performance of the neural network models\"\n",
    "\n",
    "- toc: true\n",
    "- comments: true\n",
    "- categories: [pytorch, FLOPS, ODML]\n",
    "- image: images/flops_logo.png\n",
    "- author: Prince"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f32a2f-e42b-4617-8060-529a95f72f27",
   "metadata": {},
   "source": [
    "# Parameters to be measured\n",
    "\n",
    "## Inference time\n",
    "- How much time did the forward inference took for the model. \n",
    "- 1 / inference time = FPS we can get from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7305e-e088-4e4b-9e19-e42fc9dc5965",
   "metadata": {},
   "source": [
    "## FLOPs: Floating Point Operations\n",
    "- Calculate the total floating operations - Addtion, Subtraction, Multiplication, Division \n",
    "- Large memory doesn't mean that FLOP's are higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703797cb-9d3c-4e7b-92dd-8ecf6abc4f9c",
   "metadata": {},
   "source": [
    "## FLOPS\n",
    "- Floating Point Operations per Second\n",
    "- The higher number of operations per second, the faster inference for the model\n",
    "\n",
    "|Name | Unit | Value |\n",
    "|-----|------|-------|\n",
    "|kiloFLOPS|kFLOPS|10^3|\n",
    "|megaFLOPS|MFLOPS|10^6|\n",
    "|gigaFLOPS|GFLOPS|10^9|\n",
    "|teraFLOPS|TFLOPS|10^12|\n",
    "|petaFLOPS|PFLOPS|10^15|\n",
    "|exaFLOPS|EFLOPS|10^18|\n",
    "|zettaFLOPS|ZFLOPS|10^21|\n",
    "|yottaFLOPS|YFLOPS|10^24|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21923c99-43d1-4802-9644-382eb1216d10",
   "metadata": {},
   "source": [
    "## MACs\n",
    "- Multiply-Accumulate computations\n",
    "- In a neuron, multiplication and addition operations happen in most of the cases.\n",
    "\n",
    "  **W1 * I1 + W2 * I2 + W3 * I3**\n",
    "  \n",
    "  \n",
    "- NOTE: **1 MAC = 2 FLOPs**\n",
    "\n",
    "  Since the operation consist's of 1 multiply and 1 addition\n",
    "  \n",
    "- NOTE: **dot product = n multiplications + n - 1 additions = 2n - 1 FLOPs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac29539-5853-4bba-a9ec-9c18ea09f818",
   "metadata": {},
   "source": [
    "# How to calculate?\n",
    "- We can actually define each layer computation in terms of it operations\n",
    "- Here we are considering the batch size to be 1. If the batch size increases, the flops also linearly increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789242d-90e6-4727-b48c-8e313d6b58ba",
   "metadata": {},
   "source": [
    "## Fully Connected Layer\n",
    "- **LAYER = (INPUT NODES) * (OUTPUT NODES) + BIAS**\n",
    "\n",
    "  where * is the dot product discussed previously.\n",
    "\n",
    "- So, For calculating FLOPs, we are just multiplying input and output. We can also add bias term, but for approximation we can leave it out.\n",
    "\n",
    "```\n",
    "\n",
    "def _linear_flops(module, inp, out):\n",
    "    mul = module.in_features\n",
    "    add = module.in_features - 1\n",
    "    total_ops = (mul + add) * out.numel()\n",
    "    return total_ops\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48e087-060f-4608-a92f-65fe0348d88e",
   "metadata": {},
   "source": [
    "## Activations \n",
    "\n",
    "- Most of the activations don't come with any overhead of multiplication but they do have some simpler arithemetic operation to it.\n",
    "\n",
    "### RELU\n",
    "\n",
    "- **LAYER = INPUT NODES**\n",
    "\n",
    "```\n",
    "# y = max(x,0) \n",
    "\n",
    "def _relu_flops(module, inp, out):\n",
    "    return inp.numel()\n",
    "```\n",
    "\n",
    "\n",
    "### Tanh\n",
    "\n",
    "- **LAYER = INPUT NODES * 5**\n",
    "\n",
    "```\n",
    "# y = e^(x) - e^(-x) / e^(x) + e^(-x)\n",
    "\n",
    "def _tanh_flops(module, inp, out):\n",
    "    # exp, exp^-1, sub, add, div for each element\n",
    "    total_ops = 5 * inp.numel()\n",
    "    return total_ops\n",
    "```\n",
    "\n",
    "### sigmoid\n",
    "\n",
    "\n",
    "- **LAYER = INPUT NODES * 4**\n",
    "\n",
    "```\n",
    "# y = 1 / (1 + e^(-x)) \n",
    "\n",
    "\n",
    "def _sigmoid_flops(module, inp, out):\n",
    "    # negate, exp, add, div for each element\n",
    "    total_ops = 4 * inp.numel()\n",
    "    return total_ops\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5441e-466c-4d20-b77d-8c3bb76365e9",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "\n",
    "- Depends on type of Pooling and Stride\n",
    "\n",
    "### MaxPool 1D, 2D, 3D\n",
    "\n",
    "- **LAYER = Max(INPUT NODES)**\n",
    "\n",
    "```\n",
    "# Same as output\n",
    "\n",
    "def _maxpool_flops(module, inp, out):\n",
    "    total_ops = out.numel()\n",
    "    return total_ops\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Average MaxPool 1D, 2D, 3D\n",
    "\n",
    "- **LAYER = Average(INPUT NODES)**\n",
    "\n",
    "```\n",
    "# Same as output with kernel size\n",
    "\n",
    "def _avgpool_flops(module, inp, out):\n",
    "    # pool: kernel size, avg: 1\n",
    "    kernel_ops = _torch.prod(_torch.Tensor([module.kernel_size]))\n",
    "    total_ops = (kernel_ops + 1) * out.numel()\n",
    "    return total_ops\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c07340-3727-4a0d-853b-0da9bbf36235",
   "metadata": {},
   "source": [
    "## DropOut\n",
    "\n",
    "- **LAYER = probability of dropout * INPUT**\n",
    "- Can be considered as Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7fdf93-a6a8-404f-a3eb-0c0928896240",
   "metadata": {},
   "source": [
    "## Batch Normalization : Only while training\n",
    "\n",
    "- **LAYER = gamma * (y - mean) / sqrt(variance + epsilon) + beta**\n",
    "\n",
    "```\n",
    "# 4 * number of input \n",
    "\n",
    "def _bn_flops(module, inp, out):\n",
    "    nelements = inp.numel()\n",
    "    # subtract, divide, gamma, beta\n",
    "    total_ops = 4 * nelements\n",
    "    return total_ops\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bf12e-e9a2-4a5a-a412-68c649bea2fd",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "- **LAYER = e^(Zi)/ SUM(e^(Zi))**\n",
    "\n",
    "```\n",
    "def _softmax_flops(module, inp, out):\n",
    "    batch_size, nfeatures = inp.size()\n",
    "    # exp: nfeatures, add: nfeatures-1, div: nfeatures\n",
    "    total_ops = batch_size * (3 * nfeatures - 1)\n",
    "    return total_ops\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26b64b-6a77-4d30-811a-d886ef64d1c6",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "- **LAYER = Number of Kernel x Kernel Shape x Output Shape**\n",
    "\n",
    "```\n",
    "\n",
    "def _convNd_flops(module, inp, out):\n",
    "    kernel_ops = module.weight.size()[2:].numel()  # k_h x k_w\n",
    "    bias_ops = 1 if module.bias is not None else 0\n",
    "    # (batch x out_c x out_h x out_w) x  (in_c x k_h x k_w + bias)\n",
    "    total_ops = out.nelement() * \\\n",
    "        (module.in_channels // module.groups * kernel_ops + bias_ops)\n",
    "    return total_ops\n",
    "\n",
    "```\n",
    "\n",
    "### Depthwise convolution\n",
    "\n",
    "- Filter and input is broken channel-wise and convolved separately. After that, they are stacked together. [Example](https://miro.medium.com/max/519/1*Esdvt3HLoEQFen94x29Z0A.png)\n",
    "\n",
    "\n",
    "- Number of operations are reduced here:\n",
    "\n",
    "  **LAYER = Number of Kernel x Kernel Shape x Output Shape(without channel)**\n",
    "\n",
    "### Pointwise convolution\n",
    "\n",
    "- 1x1 Filter is applied on each pixel of input with same channel. [Example](https://app.dropinblog.com/uploaded/blogs/34241363/files/deep-learning-optimization_2.jpeg)\n",
    "\n",
    "- Number of operations are reduced here:\n",
    "\n",
    "  **LAYER = Number of Kernel x Kernel Shape(1x1) x Output Shape(without channel)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c48221-6e4d-4ea8-801d-ee0c6e2a9469",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e5867-7ba3-4571-b624-fe20aa599f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
